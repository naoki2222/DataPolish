{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm \n",
    "import openpyxl\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_nameのファイルに隣接するノードのリストを出力\n",
    "def adjacent_node(file_name, java_module):\n",
    "    \n",
    "    adjacent_list = []\n",
    "    for edge in java_module:\n",
    "        if file_name == edge[0]:\n",
    "            adjacent_list.append(edge[1])\n",
    "\n",
    "    return adjacent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#類似度計算\n",
    "def similarity(listA, listB):\n",
    "    return len(set(listA) & set(listB)) / len(set(listA) | set(listB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#類似度の最小値を計算\n",
    "def min_similarity(file_list, java_module):\n",
    "    \n",
    "    min_sim = 1\n",
    "    for f1 in tqdm(file_list):\n",
    "        for f2 in file_list:\n",
    "            an1 = adjacent_node(f1, java_module)\n",
    "            an2 = adjacent_node(f2, java_module)\n",
    "            if len(set(an1) | set(an2)) == 0:\n",
    "                continue\n",
    "            sim = similarity(an1, an2)\n",
    "            if sim < min_sim and sim != 0 and f1 != f2:\n",
    "                min_sim = sim\n",
    "    return min_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データ研磨\n",
    "#java_moduleは無向グラフを使う\n",
    "#出力も無向グラフになる\n",
    "def data_polish(java_line ,java_module, threshold):\n",
    "\n",
    "    adjacent_list = []\n",
    "    new_java_module = []\n",
    "\n",
    "    #隣接するファイルの作成\n",
    "    for i in java_line:\n",
    "        sub_adjacent = [i[0]]\n",
    "        for j in java_module:\n",
    "            if i[0] == j[1]:\n",
    "                sub_adjacent.append(j[0])\n",
    "        adjacent_list.append([i[0],list(set(sub_adjacent))])\n",
    "        \n",
    "    #データ研磨\n",
    "    for i in tqdm(adjacent_list):\n",
    "        for j in adjacent_list:\n",
    "            if threshold <= similarity(i[1],j[1]) and i != j:\n",
    "                new_java_module.append([i[0],j[0]])\n",
    "    \n",
    "    return new_java_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データ研磨の実行\n",
    "def data_polish_repeat(java_line ,java_module, threshold, times):\n",
    "    \n",
    "    new_java_module = data_polish(java_line ,java_module, threshold)\n",
    "    \n",
    "    for i in range(times-1):\n",
    "        new_java_module = data_polish(java_line, new_java_module, threshold)\n",
    "        \n",
    "    return new_java_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#与えられたファイル名が入っている閉グラフを検出\n",
    "def single_creek_detection(file_name,java_module):  \n",
    "    \n",
    "    group_Z = [file_name]\n",
    "\n",
    "    while True:\n",
    "        adj_list = []\n",
    "        for i in group_Z:\n",
    "            for j in java_module:\n",
    "                if (i == j[1]) and (j[0] not in group_Z) and (j[0] not in adj_list):\n",
    "                    adj_list = adj_list + [j[0]]\n",
    "                if (i == j[0]) and (j[1] not in group_Z) and (j[1] not in adj_list):\n",
    "                    adj_list = adj_list + [j[1]]\n",
    "\n",
    "        if adj_list == []:\n",
    "            break\n",
    "\n",
    "        adj_list = adj_list + group_Z\n",
    "\n",
    "        if set(group_Z) != set(adj_list):\n",
    "            group_Z = adj_list\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return group_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#閉グラフをすべて検出\n",
    "def creek_detection(java_commit, java_module):\n",
    "    \n",
    "    commit_files = []\n",
    "    commit_module = []\n",
    "    \n",
    "    for i in java_commit:\n",
    "        commit_files.append(i[0])\n",
    "\n",
    "    for i in java_module:\n",
    "        if i[0] in commit_files and i[1] in commit_files:\n",
    "            commit_module.append(i)\n",
    "\n",
    "    creek = []\n",
    "    creek_list = []\n",
    "    exclude_files = []\n",
    "\n",
    "    for file_name in tqdm(commit_files):\n",
    "        if file_name not in exclude_files:\n",
    "            creek = single_creek_detection(file_name,commit_module)\n",
    "            exclude_files = exclude_files + creek\n",
    "            creek_list.append(creek)\n",
    "\n",
    "    return creek_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commit_filter(java_commit, creek):\n",
    "    \n",
    "    commit_files = []\n",
    "\n",
    "    for i in java_commit:\n",
    "        commit_files.append(i[0])\n",
    "    \n",
    "    for c in creek:\n",
    "        union = set(c) & set(commit_files)\n",
    "        if union != set():\n",
    "            print('クラスタの大きさ' + str(len(c)))\n",
    "            print(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def include_commit_creek(java_commit, creek):\n",
    "    \n",
    "    commit_files = []\n",
    "\n",
    "    for i in java_commit:\n",
    "        commit_files.append(i[0])\n",
    "    \n",
    "    for c in creek:\n",
    "        union = set(c) & set(commit_files)\n",
    "        if union != set():\n",
    "            print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_file_creek(file_name, creek):\n",
    "    \n",
    "    for c in creek:\n",
    "        if file_name in c:\n",
    "            print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('java_line.bin', 'rb') as f:\n",
    "    java_line = pickle.load(f) # load commit_set\n",
    "with open('java_commit.bin', 'rb') as f:\n",
    "    java_commit = pickle.load(f)\n",
    "with open('prob_list.bin', 'rb') as f:\n",
    "    prob_list = pickle.load(f)\n",
    "with open('java_module.bin', 'rb') as f:  #astの方\n",
    "    java_module = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(java_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_module = []\n",
    "commit_files = []\n",
    "\n",
    "for i in java_commit:\n",
    "    commit_files.append(i[0])\n",
    "\n",
    "for i in java_module:\n",
    "    if i[0] in commit_files and i[1] in commit_files:\n",
    "        commit_module.append(i)\n",
    "\n",
    "len(commit_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in java_commit:\n",
    "    if adjacent_node(j[0], commit_module) == []:\n",
    "        print(j[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = 10\n",
    "threshold = 0.25\n",
    "polished_graph = data_polish_repeat(java_line , java_module, threshold, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "creek = creek_detection(java_line, polished_graph)\n",
    "creek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_filter(java_commit, creek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_commit_creek(java_commit, creek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'java/javax/el/ImportHandler.java'\n",
    "select_file_creek(file_name, creek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('creek_java_module.bin', 'wb') as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#結果の保存\n",
    "result_path = '../result'\n",
    "\n",
    "if not os.path.exists(result_path):\n",
    "    os.mkdir(result_path)\n",
    "    print(result_path + 'を作成しました.')\n",
    "\n",
    "#保存ファイル名の決定\n",
    "Excel_name = 'result_'+str(times)+'回_dependency'\n",
    "Excel_name2 = 'result_'+str(times)+'回_creek'\n",
    "\n",
    "#データフレームへの変換\n",
    "df = pd.DataFrame(polished_graph, columns=['source','target'])\n",
    "df2 = pd.DataFrame(creek)\n",
    "\n",
    "\n",
    "#エクセルデータとして結果を保存\n",
    "df.to_excel(result_path+'/'+Excel_name+'.xlsx', sheet_name='new_sheet_name')\n",
    "df2.to_excel(result_path+'/'+Excel_name2+'.xlsx', sheet_name='new_sheet_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#結果の保存\n",
    "result_path = '../result'\n",
    "\n",
    "#ノードとエッジの保存\n",
    "Excel_name = 'edge'  #-->エッジの保存\n",
    "df = pd.DataFrame(commit_module, columns=['source','target'])\n",
    "df.to_excel(result_path+'/'+Excel_name+'.xlsx', sheet_name='new_sheet_name')\n",
    "\n",
    "# コミットのファイル\n",
    "node = []\n",
    "for i in java_commit:\n",
    "    node.append([i[0],i[0]])\n",
    "\n",
    "Excel_name = 'node'  #-->エッジの保存\n",
    "df2 = pd.DataFrame(node, columns=['id','label'])\n",
    "df2.to_excel(result_path+'/'+Excel_name+'.xlsx', sheet_name='new_sheet_name')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
